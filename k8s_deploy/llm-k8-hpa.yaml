# llm-k8-hpa.yaml
# kubectl apply -f llm-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-app-hpa
spec:
  scaleTargetRef: # Tells HPA which workload to scale
    apiVersion: apps/v1
    kind: Deployment
    name: llm-app-deployment
  minReplicas: 1      # Minimum number of replicas
  maxReplicas: 3      # Maximum number of replicas HPA can scale to
  metrics:
  - type: Resource    # Scaling based on a resource metric
    resource:
      name: cpu       # The resource is CPU
      target:
        type: Utilization # Target a certain utilization percentage
        averageUtilization: 70 # Target 70% average CPU utilization across all Pods
                               # If average CPU > 70%, HPA will scale up
                               # If average CPU < 70%, HPA will scale down
  # Optional: Behavior for scaling up and down (to control speed/aggressiveness)
  # behavior:
  #   scaleDown:
  #     stabilizationWindowSeconds: 300 # Wait 5 mins before scaling down after a scale up
  #     policies:
  #     - type: Percent
  #       value: 100 # Remove all necessary pods to reach target
  #       periodSeconds: 15
  #   scaleUp:
  #     stabilizationWindowSeconds: 0 # Scale up immediately
  #     policies:
  #     - type: Percent
  #       value: 100 # Add 100% of current replicas (doubles)
  #       periodSeconds: 15
  #     - type: Pods
  #       value: 4 # Or add up to 4 pods
  #       periodSeconds: 15
  #     selectPolicy: Max # Take the max of the two policies
